{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEL4id13e8Uc"
      },
      "source": [
        "IMDB MOVIE REVIEW SENTIMENT ANALYSIS WITH TENSORFLOW AND BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qdoq_GZfRVm"
      },
      "source": [
        "1) Connecting Kaggle IMDB Review Dataset By Using Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IO58Vr7Ab15N"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ntHht0pgcYLL"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "op_HtIxrcp35",
        "outputId": "5346a0af-a979-4b80-e4cb-a45ee026a5a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-97b7fd08-b437-406f-9f47-d99e45039466\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-97b7fd08-b437-406f-9f47-d99e45039466\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "938oA7oLctgQ"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zG_9Of3xc0KW"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ksceS_KEc3uf"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzDmhSRac_1w",
        "outputId": "eb988237-93a2-40e3-8ca5-6110937eab6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                             title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "meirnizri/covid19-dataset                                       COVID-19 Dataset                                      5MB  2022-11-13 15:47:17           9518        281  1.0              \n",
            "mattop/alcohol-consumption-per-capita-2016                      Alcohol Consumption Per Capita 2016                   4KB  2022-12-09 00:03:11            985         35  1.0              \n",
            "michals22/coffee-dataset                                        Coffee dataset                                       24KB  2022-12-15 20:02:12           1095         41  1.0              \n",
            "thedevastator/jobs-dataset-from-glassdoor                       Salary Prediction                                     3MB  2022-11-16 13:52:31           6076        134  1.0              \n",
            "swaptr/fifa-world-cup-2022-match-data                           FIFA World Cup 2022 Match Data                        7KB  2022-12-15 02:50:56            881         25  1.0              \n",
            "thedevastator/uncovering-wage-disparities-in-pennsylvania-s-hi  Higher Education Wages                              223KB  2022-12-04 15:42:36            737         30  1.0              \n",
            "elmoallistair/emmision-of-air-pollutants                        Emmision of Air Pollutants                            1MB  2022-12-08 07:25:01            773         27  1.0              \n",
            "whenamancodes/predict-diabities                                 Predict Diabetes                                      9KB  2022-11-09 12:18:49           6384        108  1.0              \n",
            "mvieira101/global-cost-of-living                                Global Cost of Living                                 1MB  2022-12-03 16:37:53           2472         56  0.9705882        \n",
            "tirendazacademy/fifa-world-cup-2022-tweets                      FIFA World Cup 2022 Tweets                            1MB  2022-12-08 19:43:37            733         31  1.0              \n",
            "thedevastator/australian-housing-data-1000-properties-sampled   Australian Housing Prices                            51KB  2022-11-28 13:19:40           1006         25  1.0              \n",
            "die9origephit/fifa-world-cup-2022-complete-dataset              Fifa World Cup 2022: Complete Dataset                 7KB  2022-12-15 18:44:57           1309         62  0.9411765        \n",
            "laibaanwer/superstore-sales-dataset                             SuperStore Sales Dataset                              2MB  2022-12-07 08:53:32            924         28  1.0              \n",
            "madhurpant/world-deaths-and-causes-1990-2019                    World Deaths and Causes (1990 - 2019)               442KB  2022-11-29 07:09:27           1746         39  1.0              \n",
            "prosperchuks/health-dataset                                     Diabetes, Hypertension and Stroke Prediction        750KB  2022-12-13 13:15:19           2796         64  1.0              \n",
            "akshaydattatraykhare/data-for-admission-in-the-university       Data for Admission in the University                  4KB  2022-10-27 11:05:45           9431        202  1.0              \n",
            "thedevastator/cancer-patients-and-air-pollution-a-new-link       Lung Cancer Prediction                               7KB  2022-11-14 13:40:40           3660         76  1.0              \n",
            "catherinerasgaitis/mxmh-survey-results                          Music & Mental Health Survey Results                 22KB  2022-11-21 10:03:12           2415         52  1.0              \n",
            "tusharaggarwal27/2022-dec-world-populations                     World Population By Countries ((1970)) to ((2050))   19KB  2022-12-08 02:23:40            877         39  1.0              \n",
            "williecosta/economic-guide-to-college-majors                    Economic Guide to College Majors (data from 538)     11KB  2022-12-01 00:03:23            631         26  1.0              \n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeWKjR1kdpcz",
        "outputId": "12442a67-1bee-4432-e3b7-25fcb24805ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            "100% 25.7M/25.7M [00:01<00:00, 35.5MB/s]\n",
            "100% 25.7M/25.7M [00:01<00:00, 22.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llgQODI6d734",
        "outputId": "7c8af5f0-e1cd-486e-ef5b-923afbaf8543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ]
        }
      ],
      "source": [
        "! unzip imdb-dataset-of-50k-movie-reviews.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngneuLU9fktF"
      },
      "source": [
        "2) Importing necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaafM1JigNWA",
        "outputId": "58ffd986-abe4-4097-8df0-435367dfcb5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-tensorflow\n",
            "  Downloading bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from bert-tensorflow) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.4\n"
          ]
        }
      ],
      "source": [
        "#!pip install bert-tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XKpNh0rTecME"
      },
      "outputs": [],
      "source": [
        "# Regular imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm # for progress bar\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tensorflow Import\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "# pd.set_option('max_rows', 99999)\n",
        "# pd.set_option('max_colwidth', 400)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z5STbng1gFM5"
      },
      "outputs": [],
      "source": [
        "movie_reviews = pd.read_csv(\"IMDB Dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "odP-AwAEg-Hm",
        "outputId": "90491974-7d92-4ad1-8412-6cb70cb0a7d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I saw this movie when I was about 12 when it c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The cast played Shakespeare.&lt;br /&gt;&lt;br /&gt;Shakes...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>This a fantastic movie of three prisoners who ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Some films just simply should not be remade. T...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I remember this film,it was the first film i h...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>An awful film! It must have been up against so...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review sentiment\n",
              "0   One of the other reviewers has mentioned that ...  positive\n",
              "1   A wonderful little production. <br /><br />The...  positive\n",
              "2   I thought this was a wonderful way to spend ti...  positive\n",
              "3   Basically there's a family where a little boy ...  negative\n",
              "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5   Probably my all-time favorite movie, a story o...  positive\n",
              "6   I sure would like to see a resurrection of a u...  positive\n",
              "7   This show was an amazing, fresh & innovative i...  negative\n",
              "8   Encouraged by the positive comments about this...  negative\n",
              "9   If you like original gut wrenching laughter yo...  positive\n",
              "10  Phil the Alien is one of those quirky films wh...  negative\n",
              "11  I saw this movie when I was about 12 when it c...  negative\n",
              "12  So im not a big fan of Boll's work but then ag...  negative\n",
              "13  The cast played Shakespeare.<br /><br />Shakes...  negative\n",
              "14  This a fantastic movie of three prisoners who ...  positive\n",
              "15  Kind of drawn in by the erotic scenes, only to...  negative\n",
              "16  Some films just simply should not be remade. T...  positive\n",
              "17  This movie made it into one of my top 10 most ...  negative\n",
              "18  I remember this film,it was the first film i h...  positive\n",
              "19  An awful film! It must have been up against so...  negative"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_reviews.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hZ384Wohjg6"
      },
      "source": [
        "To be able to use the text, we have to prepare it accordingly. In the first step, we create a function that removes the line breaks and other HTML leftovers from the text. In this step, we also filter out other text impurities using Regular Expressions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O9LlX2jXg-tO"
      },
      "outputs": [],
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "  return TAG_RE.sub('', text)\n",
        "\n",
        "def preprocess_tags(sen):\n",
        "  # Removing html tags\n",
        "  sentence = remove_tags(sen)\n",
        "\n",
        "  # Remove punctuations and numbers\n",
        "  sentence = re.sub('[^a-zA-Z]',' ', sentence)\n",
        "\n",
        "  # Single character removal\n",
        "  sentence = re.sub(r's+[a-zA-Z]s+',' ', sentence)\n",
        "\n",
        "  # Removing multiple spaces\n",
        "  sentence = re.sub(r's+', ' ', sentence)\n",
        "\n",
        "  return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "movie_reviews['review'] = movie_reviews['review'].apply(preprocess_tags)\n",
        "movie_reviews['sentiment'] = movie_reviews['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds = tf.data.Dataset.from_tensor_slices((\n",
        "#     dict(movie_reviews['review']),\n",
        "#     movie_reviews['sentiment'],\n",
        "\n",
        "# ))\n",
        "\n",
        "ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            movie_reviews['review'].values,\n",
        "            tf.cast(movie_reviews['sentiment'].values, tf.int32)\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=50000>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds.__len__()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to apply BERT tokenizer to use pre-trained tokenizer and then prepare data for BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 512\n",
        "def convert_to_feature(review):\n",
        "    return tokenizer.encode_plus(review,\n",
        "                    add_special_tokens=True,\n",
        "                    max_length=max_length,\n",
        "                    pad_to_max_length=True,\n",
        "                    return_attention_mask=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transforming raw data to suitable form for BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_to_dict(input_ids,attention_mask, token_type_ids, label):\n",
        "    return {\n",
        "        \"input_ids\":input_ids,\n",
        "        \"attention_mask\":attention_mask,\n",
        "        \"token_type_ids\":token_type_ids\n",
        "    },label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_reviews(ds, limit=-1):\n",
        "    input_ids_list=[]\n",
        "    token_type_ids_list=[]\n",
        "    attention_mask_list=[]\n",
        "    label_list=[]\n",
        "    \n",
        "    if limit >0:\n",
        "        ds = ds.take(limit)\n",
        "    \n",
        "    for review, label in tfds.as_numpy(ds):\n",
        "        bert_input = convert_to_feature(review.decode())\n",
        "        input_ids_list.append(bert_input['input_ids'])\n",
        "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append(label)\n",
        "    \n",
        "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_to_dict)\n",
        "    \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spliting dataset as train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n"
          ]
        }
      ],
      "source": [
        "#ds_size = print([i for i,_ in enumerate(ds)][-1] + 1)\n",
        "ds_size = int(ds.__len__())\n",
        "print(ds_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "train_size = int(ds_size*0.8)\n",
        "test_size = int(ds_size*0.2)\n",
        "print(train_size)\n",
        "print(test_size)\n",
        "\n",
        "ds_train = ds.take(train_size)\n",
        "ds_test = ds.skip(train_size).take(test_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to create out train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "c:\\Anaconda3\\envs\\tf_latest\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "batch_size=6\n",
        "ds_train_encoded = encode_reviews(ds_train).shuffle(10000).batch(batch_size)\n",
        "ds_test_encoded = encode_reviews(ds_test).shuffle(10000).batch(batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to initialize BERT model for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf_latest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "9290f6728956ec549d123a89345e0453b7d42487a32ec64bbdcb1cf240ed80eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
